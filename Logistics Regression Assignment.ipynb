{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#THEORETICAL"
      ],
      "metadata": {
        "id": "x9KqWkiIzC1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "Answer: ### **Logistic Regression vs. Linear Regression**  \n",
        "\n",
        "#### **What is Logistic Regression?**  \n",
        "Logistic Regression is a **supervised learning algorithm** used for **classification tasks**. It predicts the probability of a categorical outcome (usually **binary classification**, i.e., 0 or 1). The algorithm uses the **sigmoid (logistic) function** to transform linear predictions into probabilities.\n",
        "\n",
        "#### **Difference Between Logistic and Linear Regression**  \n",
        "\n",
        "| Feature            | **Linear Regression** | **Logistic Regression** |\n",
        "|--------------------|----------------------|--------------------------|\n",
        "| **Purpose**        | Predicts **continuous values** | Predicts **categorical classes (0/1)** |\n",
        "| **Equation**       | \\( Y = \\beta_0 + \\beta_1 X \\) | \\( P(Y=1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}} \\) |\n",
        "| **Function Used**  | No transformation (direct prediction) | Uses **sigmoid function** to map predictions between 0 and 1 |\n",
        "| **Output**         | Any real number (-∞ to ∞) | Probability (0 to 1) |\n",
        "| **Use Case**       | Regression problems (e.g., predicting house prices) | Classification problems (e.g., spam detection) |\n",
        "| **Loss Function**  | Mean Squared Error (MSE) | Log Loss (Binary Cross-Entropy) |\n",
        "\n",
        "#Ques 2. What is the mathematical equation of Logistic Regression?\n",
        "Answer: **Mathematical Equation of Logistic Regression**  \n",
        "\n",
        "Logistic Regression predicts the probability of a class label (e.g., 0 or 1) using the **sigmoid function**. The equation is:\n",
        "\n",
        "\\[\n",
        "P(Y=1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n)}}\n",
        "\\]\n",
        "\n",
        "where:  \n",
        "- \\( P(Y=1 | X) \\) = Probability of class **1** given input \\( X \\).  \n",
        "- \\( \\beta_0 \\) = **Intercept (bias term)**.  \n",
        "- \\( \\beta_1, \\beta_2, ..., \\beta_n \\) = **Coefficients (weights)**.  \n",
        "- \\( X_1, X_2, ..., X_n \\) = **Independent variables (features)**.  \n",
        "- \\( e \\) = Euler’s number (~2.718), base of the natural logarithm.\n",
        "\n",
        "### **Simplified Form**\n",
        "Using **z** as the linear combination:\n",
        "\n",
        "\\[\n",
        "z = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n\n",
        "\\]\n",
        "\n",
        "The logistic (sigmoid) function transforms \\( z \\) into a probability:\n",
        "\n",
        "\\[\n",
        "P(Y=1) = \\frac{1}{1 + e^{-z}}\n",
        "\\]\n",
        "\n",
        "#Ques 3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "Answer: **Why Do We Use the Sigmoid Function in Logistic Regression?**  \n",
        "\n",
        "The **sigmoid function** is used in **Logistic Regression** because it maps any real number into a probability range **(0 to 1)**, making it ideal for classification problems.\n",
        "\n",
        "#### **Sigmoid Function Formula:**\n",
        "\\[\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "\\]\n",
        "where:  \n",
        "- \\( z = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n \\) (linear combination of input features)\n",
        "- \\( e \\) is Euler's number (**~2.718**)\n",
        "\n",
        "**Why Use Sigmoid?**\n",
        "1. **Probability Output (0 to 1)**  \n",
        "   - Converts raw predictions into probabilities, helping in classification.\n",
        "   \n",
        "2. **Threshold-Based Decision Making**  \n",
        "   - If \\( \\sigma(z) > 0.5 \\), classify as **1**; otherwise, classify as **0**.\n",
        "\n",
        "3. **Smooth and Differentiable**  \n",
        "   - Ensures smooth gradient descent updates during optimization.\n",
        "\n",
        "4. **Handles Large Positive and Negative Values**  \n",
        "   - **For large \\( z \\)** → \\( \\sigma(z) \\approx 1 \\)  \n",
        "   - **For small \\( z \\)** → \\( \\sigma(z) \\approx 0 \\)  \n",
        "\n",
        "#Ques 4. What is the cost function of Logistic Regression?\n",
        "Answer: **Cost Function of Logistic Regression**  \n",
        "\n",
        "In **Logistic Regression**, we use the **log loss function (Binary Cross-Entropy Loss)** instead of **Mean Squared Error (MSE)** because MSE does not work well for classification problems.  \n",
        "\n",
        "**Cost Function Formula:**\n",
        "\\[\n",
        "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
        "\\]\n",
        "where:  \n",
        "- \\( m \\) = Number of training examples  \n",
        "- \\( y_i \\) = Actual class label (0 or 1)  \n",
        "- \\( \\hat{y}_i \\) = Predicted probability of class **1**  \n",
        "- \\( \\log \\) = Natural logarithm  \n",
        "\n",
        "**Why Use Log Loss Instead of MSE?**  \n",
        "1. **MSE leads to non-convex optimization**  \n",
        "   - Causes slow convergence and local minima issues.  \n",
        "2. **Log Loss is convex**  \n",
        "   - Ensures a **global minimum**, making gradient descent work efficiently.  \n",
        "3. **Punishes incorrect confident predictions more**  \n",
        "   - If the model is very wrong, it receives a **higher penalty**.  \n",
        "\n",
        "#Ques 5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "Answer: **Regularization in Logistic Regression**  \n",
        "\n",
        "**Regularization** is a technique used in **Logistic Regression** to **prevent overfitting** by adding a penalty term to the cost function. It helps keep model coefficients **small**, improving generalization on unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "### **Types of Regularization**  \n",
        "1. **L1 Regularization (Lasso Regression)**  \n",
        "   - Adds the **absolute values** of coefficients as a penalty:  \n",
        "     \\[\n",
        "     J(\\theta) = -\\frac{1}{m} \\sum [y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})] + \\lambda \\sum |\\theta_j|\n",
        "     \\]  \n",
        "   - Helps with **feature selection** by shrinking some coefficients to **zero**.\n",
        "\n",
        "2. **L2 Regularization (Ridge Regression)**  \n",
        "   - Adds the **squared values** of coefficients as a penalty:  \n",
        "     \\[\n",
        "     J(\\theta) = -\\frac{1}{m} \\sum [y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})] + \\lambda \\sum \\theta_j^2\n",
        "     \\]  \n",
        "   - Prevents overfitting but **does not shrink coefficients to zero**.\n",
        "\n",
        "3. **Elastic Net Regularization**  \n",
        "   - Combines **L1 and L2**:  \n",
        "     \\[\n",
        "     J(\\theta) = -\\frac{1}{m} \\sum [y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})] + \\lambda_1 \\sum |\\theta_j| + \\lambda_2 \\sum \\theta_j^2\n",
        "     \\]  \n",
        "   - Helps when **features are correlated**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why is Regularization Needed?**  \n",
        "**Prevents Overfitting** → Avoids models that memorize training data.  \n",
        "**Improves Generalization** → Helps model work well on new data.  \n",
        "**Reduces Model Complexity** → Shrinks coefficients, making the model simpler.  \n",
        "**Handles Multicollinearity** → L2 helps when features are highly correlated.  \n",
        "\n",
        "#Ques 6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "Answer: **Difference Between Lasso, Ridge, and Elastic Net Regression**  \n",
        "\n",
        "Regularization techniques help **prevent overfitting** in **Logistic Regression** by penalizing large coefficients.  \n",
        "\n",
        "| **Regularization Type** | **Penalty Term** | **Effect on Coefficients** | **Use Case** |\n",
        "|----------------------|----------------------|----------------------|------------------|\n",
        "| **Lasso (L1)**      | \\( \\lambda \\sum |\\theta_j| \\) | Shrinks some coefficients to **zero** (feature selection). | When some features are irrelevant (helps with sparsity). |\n",
        "| **Ridge (L2)**      | \\( \\lambda \\sum \\theta_j^2 \\) | Shrinks all coefficients but **does not make them zero**. | When all features are important, and multicollinearity is present. |\n",
        "| **Elastic Net**      | \\( \\lambda_1 \\sum |\\theta_j| + \\lambda_2 \\sum \\theta_j^2 \\) | **Combination of L1 and L2**, balances feature selection and coefficient shrinkage. | When there are correlated features and feature selection is needed. |\n",
        "\n",
        "#Ques 7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "Answer:**When Should We Use Elastic Net Instead of Lasso or Ridge?**  \n",
        "\n",
        "Elastic Net is a **combination of Lasso (L1) and Ridge (L2) regularization**. It is preferred over Lasso or Ridge when:  \n",
        "\n",
        "---\n",
        "**1. Features Are Highly Correlated (Multicollinearity)**  \n",
        "- **Ridge** is good at handling correlated features but keeps all coefficients.  \n",
        "- **Lasso** tends to randomly select one feature and discard the others.  \n",
        "- **Elastic Net** selects **groups of correlated features**, preventing arbitrary feature elimination.  \n",
        "\n",
        "**Use Elastic Net when you want to retain important correlated features**.  \n",
        "\n",
        "---\n",
        "**2. You Need Feature Selection But Lasso is Too Aggressive**  \n",
        "- **Lasso (L1) shrinks some coefficients to zero**, removing features completely.  \n",
        "- If Lasso removes too many features, Elastic Net **balances between L1 (feature selection) and L2 (coefficient shrinkage)**.  \n",
        "\n",
        "**Use Elastic Net when Lasso removes too many features and Ridge keeps too many.**  \n",
        "\n",
        "---\n",
        "\n",
        "**3. You Need a Balance Between Shrinking and Eliminating Coefficients**  \n",
        "- **Ridge regression only shrinks coefficients** (never zero).  \n",
        "- **Lasso forces some coefficients to exactly zero**.  \n",
        "- **Elastic Net applies both effects**, meaning:  \n",
        "  - Small but important coefficients stay in the model.  \n",
        "  - Irrelevant features are removed.  \n",
        "\n",
        "**Use Elastic Net when you want to shrink some features and remove others.**  \n",
        "\n",
        "---\n",
        "**4. When You Have More Features than Observations (\\( p > n \\))**  \n",
        "- Lasso struggles when the number of features is greater than the number of samples.  \n",
        "- Elastic Net works better in **high-dimensional datasets**.  \n",
        "\n",
        "**Use Elastic Net when your dataset has many more features than samples.**  \n",
        "\n",
        "#Ques 8. What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "Answer: **Impact of the Regularization Parameter (λ) in Logistic Regression**  \n",
        "\n",
        "The **regularization parameter (λ)** (also called **alpha or C in scikit-learn**) controls the **strength of the penalty** applied to the model's coefficients in **Logistic Regression**. It helps balance **overfitting** and **underfitting**.\n",
        "\n",
        "**How λ Affects Logistic Regression**  \n",
        "\n",
        "1. **Small λ (Weak Regularization) → High Complexity Model**  \n",
        "   - Less penalty on large coefficients.  \n",
        "   - Model **fits training data well** (low bias).  \n",
        "   - **Risk of overfitting**, meaning poor generalization.  \n",
        "\n",
        "2. **Large λ (Strong Regularization) → Simple Model**  \n",
        "   - Penalizes large coefficients, forcing them closer to zero.  \n",
        "   - Reduces model complexity (**high bias, low variance**).  \n",
        "   - **Prevents overfitting** but may lead to underfitting.  \n",
        "\n",
        "#Ques 9. What are the key assumptions of Logistic Regression?\n",
        "Answer: **Key Assumptions of Logistic Regression**  \n",
        "\n",
        "Logistic Regression is a powerful classification algorithm, but it relies on certain **assumptions** to perform well.  \n",
        "\n",
        "---\n",
        "\n",
        "**1. The Dependent Variable is Binary (For Binary Logistic Regression)**  \n",
        "- Logistic Regression is designed for **binary classification (0 or 1)**.  \n",
        "- For **multiclass classification**, extensions like **One-vs-Rest (OvR)** or **Softmax Regression** are used.  \n",
        "\n",
        "---\n",
        "\n",
        "**2. Independence of Observations**  \n",
        "- Each data point should be **independent** of the others.  \n",
        "- **No repeated measurements** from the same individual or correlated observations.  \n",
        "\n",
        "---\n",
        "\n",
        "**3. Linearity Between Independent Variables and Log-Odds**  \n",
        "- Unlike **Linear Regression**, which assumes a direct linear relationship, **Logistic Regression assumes a linear relationship between the independent variables and the log-odds of the dependent variable**:  \n",
        "\n",
        "  \\[\n",
        "  \\log\\left(\\frac{P(Y=1)}{1 - P(Y=1)}\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n\n",
        "  \\]  \n",
        "\n",
        "- If this assumption is violated, **polynomial terms** or **interaction features** can help.\n",
        "\n",
        "---\n",
        "\n",
        "**4. No Perfect Multicollinearity**  \n",
        "- Independent variables should **not be highly correlated** with each other.  \n",
        "- If multicollinearity exists, **Ridge Regression (L2)** or **Principal Component Analysis (PCA)** can be used.  \n",
        "\n",
        "---\n",
        "\n",
        "**5. Large Sample Size for Reliable Estimates**  \n",
        "- Logistic Regression requires a **sufficient number of observations** to provide stable probability estimates.  \n",
        "- A rule of thumb: **At least 10 observations per predictor variable**.  \n",
        "\n",
        "---\n",
        "\n",
        "**6. No Extreme Outliers**  \n",
        "- Logistic Regression is sensitive to **outliers**, which can **distort coefficient estimates**.  \n",
        "- **Handling outliers:**  \n",
        "  - Use **robust scaling (log transformation, standardization)**.  \n",
        "  - Apply **regularization (L1 or L2)** to prevent large coefficient values.\n",
        "\n",
        "#Ques 10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "Answer: **Alternatives to Logistic Regression for Classification Tasks**  \n",
        "\n",
        "While **Logistic Regression** is a simple and effective classification algorithm, it may not always be the best choice, especially for **non-linear** data or complex patterns. Below are some **alternative classification algorithms**:  \n",
        "\n",
        "---\n",
        "\n",
        "**1. Decision Trees**  \n",
        "- **How it Works:** Splits data into hierarchical **decision rules**.  \n",
        "- **Pros:**  \n",
        "  Handles **non-linearity** and feature interactions well.  \n",
        "  **Interpretable** and easy to visualize.  \n",
        "- **Cons:**  \n",
        "  Prone to **overfitting** without pruning.  \n",
        "\n",
        "**Use when: Feature importance and interpretability are needed.**  \n",
        "\n",
        "---\n",
        "\n",
        "**2. Random Forest**   \n",
        "- **How it Works:** A collection of **multiple decision trees** (ensemble method).  \n",
        "- **Pros:**  \n",
        "  **Reduces overfitting** by averaging multiple trees.  \n",
        "  Works well with **imbalanced and noisy data**.  \n",
        "- **Cons:**  \n",
        "Computationally expensive for large datasets.  \n",
        "\n",
        "**Use when: You need a powerful, robust classifier.**  \n",
        "\n",
        "---\n",
        "\n",
        "**3. Support Vector Machine (SVM)**  \n",
        "- **How it Works:** Finds an **optimal decision boundary** using support vectors.  \n",
        "- **Pros:**  \n",
        "  Works well in **high-dimensional spaces**.  \n",
        "  Effective for **small and medium-sized datasets**.  \n",
        "- **Cons:**  \n",
        "  Computationally **expensive for large datasets**.  \n",
        "\n",
        "**Use when: You have high-dimensional data and need a strong decision boundary.**  \n",
        "\n",
        "---\n",
        "\n",
        "**4. k-Nearest Neighbors (k-NN)**   \n",
        "- **How it Works:** Classifies based on the **majority class of k-nearest neighbors**.  \n",
        "- **Pros:**  \n",
        "  **No training phase** (lazy learner).  \n",
        "  Works well with **small datasets**.  \n",
        "- **Cons:**  \n",
        "  **Slow for large datasets** (computationally expensive).  \n",
        "  Affected by **irrelevant features** and **imbalanced data**.  \n",
        "\n",
        "**Use when: You have small datasets and well-separated classes.**  \n",
        "\n",
        "---\n",
        "\n",
        "**5. Naïve Bayes**  \n",
        "- **How it Works:** Uses **Bayes' Theorem** to calculate class probabilities.  \n",
        "- **Pros:**  \n",
        "**Fast and efficient**, even on **large datasets**.  \n",
        "Works well with **text classification (spam detection, sentiment analysis)**.  \n",
        "- **Cons:**  \n",
        "Assumes **independent features**, which is rarely true in real-world data.  \n",
        "\n",
        "**Use when: You need a fast, probabilistic classifier, especially for text data.**  \n",
        "\n",
        "---\n",
        "\n",
        "**6. Neural Networks (Deep Learning)**\n",
        "- **How it Works:** Mimics the **human brain** using layers of artificial neurons.  \n",
        "- **Pros:**  \n",
        "**Handles complex, non-linear relationships**.  \n",
        "Works well for **image recognition, speech processing, NLP**.  \n",
        "- **Cons:**  \n",
        "**Computationally expensive** and requires **large datasets**.  \n",
        "\n",
        "**Use when: You have a large dataset and need deep feature extraction.**  \n",
        "\n",
        "#Ques 11. What are Classification Evaluation Metrics?\n",
        "Answer: **Classification Evaluation Metrics**  \n",
        "\n",
        "Classification evaluation metrics are used to measure how well a classification model, such as **Logistic Regression, Decision Trees, or SVM**, performs. Unlike regression models, classification models predict discrete labels (e.g., 0 or 1), so specialized metrics are needed to assess their accuracy, precision, recall, and overall performance.  \n",
        "\n",
        "---\n",
        "\n",
        "**1. Accuracy**   \n",
        "Accuracy measures the percentage of correctly classified instances out of the total instances. It is calculated as:  \n",
        "\\[\n",
        "Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "\\]\n",
        "where:  \n",
        "- **TP (True Positives)** → Correctly predicted positives.  \n",
        "- **TN (True Negatives)** → Correctly predicted negatives.  \n",
        "- **FP (False Positives)** → Incorrectly predicted positives.  \n",
        "- **FN (False Negatives)** → Incorrectly predicted negatives.  \n",
        "\n",
        "**Best for:** Balanced datasets.  \n",
        "**Limitation:** Misleading when classes are imbalanced (e.g., 95% accuracy in a dataset where 95% of samples belong to one class).  \n",
        "\n",
        "---\n",
        "\n",
        "**2. Precision (Positive Predictive Value)**  \n",
        "Precision measures the proportion of correctly predicted positive cases out of all predicted positives:  \n",
        "\\[\n",
        "Precision = \\frac{TP}{TP + FP}\n",
        "\\]\n",
        "High precision means fewer **false positives**.  \n",
        "\n",
        "**Best for:** When **false positives** are costly (e.g., spam detection, fraud detection).  \n",
        "**Limitation:** Ignores false negatives, which might be critical in some cases.  \n",
        "\n",
        "---\n",
        "\n",
        "**3. Recall (Sensitivity / True Positive Rate)**  \n",
        "Recall measures the proportion of actual positive cases that were correctly predicted:  \n",
        "\\[\n",
        "Recall = \\frac{TP}{TP + FN}\n",
        "\\]\n",
        "High recall means fewer **false negatives**.  \n",
        "\n",
        "**Best for:** When **false negatives** are costly (e.g., cancer detection, security alerts).  \n",
        "**Limitation:** Ignores false positives, which might be important in some cases.  \n",
        "\n",
        "---\n",
        "\n",
        "**4. F1-Score (Harmonic Mean of Precision & Recall)**   \n",
        "F1-Score balances **Precision and Recall** to provide a single performance measure:  \n",
        "\\[\n",
        "F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\n",
        "\\]\n",
        "- If **F1-score is high**, the model has both **high precision and recall**.  \n",
        "- Useful for **imbalanced datasets**, where accuracy alone is misleading.  \n",
        "\n",
        "**Best for:** When both **false positives & false negatives** matter.  \n",
        "\n",
        "---\n",
        "\n",
        "**5. ROC-AUC (Receiver Operating Characteristic - Area Under Curve)**  \n",
        "- **ROC Curve** plots **True Positive Rate (Recall) vs. False Positive Rate (1 - Specificity)**.  \n",
        "- **AUC (Area Under Curve)** measures model performance:  \n",
        "  - **AUC = 1.0** → Perfect model.  \n",
        "  - **AUC = 0.5** → Random guessing.  \n",
        "  - **AUC < 0.5** → Worse than random.  \n",
        "\n",
        "**Best for:** Evaluating probability-based models.  \n",
        "\n",
        "---\n",
        "\n",
        "**6. Log Loss (Binary Cross-Entropy Loss)**  \n",
        "Log Loss measures the **error in probability predictions**, penalizing incorrect confident predictions more heavily:  \n",
        "\\[\n",
        "LogLoss = -\\frac{1}{m} \\sum \\left[ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right]\n",
        "\\]\n",
        "Lower log loss = better model performance.  \n",
        "\n",
        "**Best for:** Logistic Regression and probability-based classifiers.  \n",
        "\n",
        "---\n",
        "\n",
        "**7. Confusion Matrix**  \n",
        "A **confusion matrix** provides a detailed breakdown of **true positives, true negatives, false positives, and false negatives**.  \n",
        "\n",
        "| Actual / Predicted | Positive (1) | Negative (0) |\n",
        "|-------------------|-------------|-------------|\n",
        "| **Positive (1)**  | TP (True Positive) | FN (False Negative) |\n",
        "| **Negative (0)**  | FP (False Positive) | TN (True Negative) |\n",
        "\n",
        "**✅ Best for:** Understanding misclassifications in detail.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison Table**  \n",
        "\n",
        "| **Metric**    | **Best for** | **When to Use** |\n",
        "|--------------|------------|----------------|\n",
        "| **Accuracy** | Balanced datasets | When FP and FN have equal costs |\n",
        "| **Precision** | Avoiding false positives | Spam filters, fraud detection |\n",
        "| **Recall** | Avoiding false negatives | Cancer detection, security alerts |\n",
        "| **F1-Score** | Imbalanced datasets | When FP & FN both matter |\n",
        "| **ROC-AUC** | Probability-based models | Comparing classification models |\n",
        "| **Log Loss** | Probability-based models | Logistic Regression evaluation |\n",
        "| **Confusion Matrix** | Analyzing errors | Understanding misclassifications |\n",
        "\n",
        "#Ques 12. How does class imbalance affect Logistic Regression?\n",
        "Answer: **How Does Class Imbalance Affect Logistic Regression?**  \n",
        "\n",
        "**Class imbalance** occurs when one class in a dataset has significantly more samples than the other(s). For example, in fraud detection, **99% of transactions might be legitimate (Class 0)** and only **1% fraudulent (Class 1)**. This imbalance can cause **Logistic Regression to favor the majority class**, leading to misleading results.  \n",
        "\n",
        "---\n",
        "\n",
        "**1. High Accuracy but Poor Performance**  \n",
        "Since Logistic Regression aims to **maximize overall accuracy**, it may simply predict the majority class most of the time. In an imbalanced dataset (e.g., 99% Class 0, 1% Class 1), the model can achieve **99% accuracy** by always predicting **Class 0**, but this is **useless** because it fails to detect any minority class instances.\n",
        "\n",
        "**Example:**  \n",
        "- **Actual labels:** [0, 0, 0, 1, 1, 1]  \n",
        "- **Predicted labels:** [0, 0, 0, 0, 0, 0]  \n",
        "- **Accuracy:** \\( \\frac{3}{6} = 50\\% \\) (Looks okay but fails to predict Class 1)  \n",
        "\n",
        "---\n",
        "\n",
        "**2. Biased Decision Boundary**  \n",
        "In a balanced dataset, Logistic Regression finds a **decision boundary** that separates both classes fairly. However, when one class dominates, the model **shifts the boundary closer to the minority class**, making it harder for the model to detect positive cases. This results in a **high false negative rate** (i.e., missing many minority class instances).  \n",
        "\n",
        "---\n",
        "\n",
        "**3. Misleading Metrics (Accuracy Paradox)**  \n",
        "With imbalanced data, **accuracy becomes a misleading metric** because it does not consider how well the model performs on the minority class. Instead, we should use:  \n",
        "- **Precision** → Measures how many predicted positive cases were actually positive.  \n",
        "- **Recall (Sensitivity)** → Measures how many actual positive cases were correctly predicted.  \n",
        "- **F1-Score** → Balances Precision and Recall.  \n",
        "- **ROC-AUC** → Evaluates probability-based predictions.\n",
        "\n",
        "---\n",
        "\n",
        "**4. Strategies to Handle Class Imbalance**  \n",
        "\n",
        "**A. Resampling Techniques**\n",
        "1. **Oversampling the Minority Class (SMOTE)**  \n",
        "   - Duplicates or generates synthetic data for the minority class to balance the dataset.  \n",
        "   - Prevents the model from ignoring minority instances.  \n",
        "\n",
        "2. **Undersampling the Majority Class**  \n",
        "   - Randomly removes samples from the majority class to balance the dataset.  \n",
        "   - Reduces training time but may lose important data.  \n",
        "\n",
        "---\n",
        "\n",
        "**B. Assigning Class Weights in Logistic Regression**\n",
        "- In **scikit-learn**, setting `class_weight='balanced'` adjusts weights inversely proportional to class frequencies.  \n",
        "  ```python\n",
        "  from sklearn.linear_model import LogisticRegression\n",
        "  model = LogisticRegression(class_weight='balanced')\n",
        "  ```\n",
        "- Ensures that the minority class contributes more to the learning process.\n",
        "\n",
        "---\n",
        "\n",
        "**C. Using Alternative Metrics for Evaluation**\n",
        "Instead of accuracy, use:\n",
        "- **Precision-Recall Curve** → Helps assess minority class performance.  \n",
        "- **F1-Score** → Balances Precision and Recall.  \n",
        "- **Confusion Matrix** → Provides insights into classification errors.\n",
        "\n",
        "#Ques 13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "Answer: ### **What is Hyperparameter Tuning in Logistic Regression?**  \n",
        "\n",
        "**Hyperparameter tuning** in Logistic Regression refers to the process of **optimizing key model parameters** to improve performance. Unlike model parameters (like **coefficients β**), which are learned from the data, **hyperparameters** must be set manually before training. The right choice of hyperparameters can significantly improve accuracy, generalization, and model efficiency.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Hyperparameters in Logistic Regression**  \n",
        "\n",
        "1. **Regularization Strength (C)**  \n",
        "   - Controls the amount of **L1 (Lasso) or L2 (Ridge) regularization** applied.  \n",
        "   - **Higher C** → Less regularization (risk of overfitting).  \n",
        "   - **Lower C** → More regularization (risk of underfitting).  \n",
        "\n",
        "2. **Penalty Type (L1, L2, or Elastic Net)**  \n",
        "   - `penalty='l1'` → Lasso regression (feature selection).  \n",
        "   - `penalty='l2'` → Ridge regression (prevents overfitting).  \n",
        "   - `penalty='elasticnet'` → Combination of L1 and L2.  \n",
        "\n",
        "3. **Solver (Optimization Algorithm)**  \n",
        "   - **lbfgs** → Best for small datasets and works with L2 regularization.  \n",
        "   - **liblinear** → Best for **L1 penalty** (sparse features).  \n",
        "   - **saga** → Works well for **large datasets** with L1 or Elastic Net.  \n",
        "\n",
        "4. **Class Weights (`class_weight`)**  \n",
        "   - Helps in **imbalanced datasets**.  \n",
        "   - `class_weight='balanced'` automatically adjusts weights based on class frequency.  \n",
        "\n",
        "#Ques 14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "Answer: **Different Solvers in Logistic Regression & When to Use Them**  \n",
        "\n",
        "Logistic Regression uses **optimization solvers** to find the best **coefficients (β)** by minimizing the **log loss function**. Different solvers have different speed, accuracy, and suitability based on dataset size and regularization type.\n",
        "\n",
        "---\n",
        "\n",
        "**1. liblinear (Good for Small Datasets, L1 & L2 Regularization)**\n",
        "- Uses **Coordinate Descent Algorithm** (efficient for small datasets).  \n",
        "- Supports **L1 (Lasso) and L2 (Ridge) regularization**.  \n",
        "- Works well when **features are sparse (many zero values)**.  \n",
        "\n",
        "**Use when:** Dataset is **small**, and you need **L1 or L2 regularization**.  \n",
        "**Avoid when:** Dataset is **large** (computationally expensive).  \n",
        "\n",
        "---\n",
        "\n",
        "**2. lbfgs (Default Solver, Best for Medium-Sized Data)**\n",
        "- Stands for **Limited-memory BFGS (Broyden-Fletcher-Goldfarb-Shanno)**.  \n",
        "- Works well for **L2 regularization** (does NOT support L1).  \n",
        "- Handles **multiclass problems (one-vs-rest by default)**.  \n",
        "\n",
        "**Use when:** Dataset is **medium-sized**, and you need **L2 regularization**.  \n",
        "**Avoid when:** Dataset is **very large** or requires **L1 regularization**.  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**3. newton-cg (Good for Large Datasets, L2 Regularization)**\n",
        "- Uses **Newton’s Conjugate Gradient** method, faster for large datasets.  \n",
        "- Supports **only L2 regularization**.  \n",
        "- More accurate for **high-dimensional datasets**.  \n",
        "\n",
        "**Use when:** Dataset is **large**, and you need **L2 regularization**.  \n",
        "**Avoid when:** You need **L1 or Elastic Net regularization**.  \n",
        "\n",
        "---\n",
        "\n",
        "**4. saga (Best for Large Datasets & L1, L2, Elastic Net)**\n",
        "- Stochastic Average Gradient Descent with L1 and L2 regularization.  \n",
        "- **Only solver** that supports **Elastic Net (L1 + L2)**.  \n",
        "- Works well for **large datasets with sparse features**.  \n",
        "\n",
        "**Use when:** Dataset is **large**, and you need **L1, L2, or Elastic Net**.  \n",
        "**Avoid when:** Dataset is **small** (computational overhead).  \n",
        "\n",
        "---\n",
        "\n",
        "**5. sag (Fast for Large Datasets, L2 Regularization)**\n",
        "- **Stochastic Average Gradient Descent**, good for **large datasets**.  \n",
        "- Works only with **L2 regularization**.  \n",
        "\n",
        "**Use when:** You have **a large dataset** and need **L2 regularization**.  \n",
        "**Avoid when:** Dataset is **small** or requires **L1 regularization**.\n",
        "\n",
        "#Ques 15. How is Logistic Regression extended for multiclass classification?\n",
        "Answer: **How is Logistic Regression Extended for Multiclass Classification?**  \n",
        "\n",
        "Logistic Regression is primarily used for **binary classification** (0 or 1). However, it can be extended to handle **multiclass classification** (more than two categories) using the following approaches:  \n",
        "\n",
        "---\n",
        "\n",
        "**1. One-vs-Rest (OvR) (One-vs-All)**\n",
        "- The **most common approach** for extending Logistic Regression to multiclass problems.  \n",
        "- The model trains **one classifier per class**, treating it as **positive (1)** while all other classes are treated as **negative (0)**.  \n",
        "- During prediction, all classifiers produce probabilities, and the **class with the highest probability** is selected.  \n",
        "\n",
        "**Pros:**  \n",
        "Simple and computationally efficient.  \n",
        "Works well with **small to medium datasets**.  \n",
        "\n",
        "**Cons:**  \n",
        "Training multiple classifiers separately can lead to inconsistencies.  \n",
        "\n",
        "---\n",
        "\n",
        "**2. Softmax Regression (Multinomial Logistic Regression)**\n",
        "- Instead of training multiple binary classifiers, **Softmax regression** trains a **single model** that predicts probabilities for **all classes simultaneously**.  \n",
        "- The **Softmax function** replaces the sigmoid function, computing the probability for each class:  \n",
        "\\[\n",
        "P(y = k | X) = \\frac{e^{\\beta_k X}}{\\sum_{j=1}^{K} e^{\\beta_j X}}\n",
        "\\]\n",
        "- The **class with the highest probability** is selected.  \n",
        "\n",
        "**Pros:**  \n",
        "More consistent than OvR as all classes are considered together.  \n",
        "Works well for **large datasets**.  \n",
        "\n",
        "**Cons:**  \n",
        "Computationally expensive compared to OvR.  \n",
        "May struggle with imbalanced datasets.  \n",
        "\n",
        "---\n",
        "\n",
        "**3. One-vs-One (OvO)**\n",
        "- Unlike OvR, which trains **K classifiers for K classes**, OvO trains **a classifier for every pair of classes** (e.g., for 3 classes, Class 1 vs. Class 2, Class 2 vs. Class 3, etc.).  \n",
        "- The final prediction is made using a **majority vote**.  \n",
        "\n",
        "**Pros:**  \n",
        "More accurate in **complex, balanced datasets**.  \n",
        "Fewer samples per classifier lead to **faster training** for large datasets.  \n",
        "\n",
        "**Cons:**  \n",
        "Requires **more models** (\\(K(K-1)/2\\)), increasing computational cost.  \n",
        "\n",
        "#Ques 16. What are the advantages and disadvantages of Logistic Regression?\n",
        "Answer: **Advantages and Disadvantages of Logistic Regression**  \n",
        "\n",
        "Logistic Regression is a widely used classification algorithm, but like any model, it has strengths and limitations.  \n",
        "\n",
        "---\n",
        "\n",
        "**Advantages of Logistic Regression**  \n",
        "\n",
        "**1. Simple and Easy to Implement**  \n",
        "Logistic Regression is straightforward to understand and implement, making it a great baseline model for classification tasks.  \n",
        "\n",
        "**2. Interpretable Model (Feature Importance)**  \n",
        "The coefficients in Logistic Regression represent the impact of each feature on the predicted outcome. This makes it useful for **understanding relationships** between variables.  \n",
        "\n",
        "**3. Computationally Efficient (Fast Training)**  \n",
        "It requires fewer computational resources compared to complex models like Random Forests or Neural Networks. It works well even on **small to medium datasets**.  \n",
        "\n",
        "**4. Works Well for Linearly Separable Data**  \n",
        "If there is a **linear relationship between features and log-odds**, Logistic Regression performs well with high accuracy.  \n",
        "\n",
        "**5. Provides Probabilistic Outputs**  \n",
        "Unlike Decision Trees, Logistic Regression outputs **probabilities**, which is useful for applications requiring confidence scores (e.g., medical diagnosis, fraud detection).  \n",
        "\n",
        "**6. Handles Multiple Classes with Extensions**  \n",
        "With **One-vs-Rest (OvR)** or **Softmax Regression**, it can handle **multiclass classification** problems.  \n",
        "\n",
        "**7. Regularization Prevents Overfitting**  \n",
        "By using **L1 (Lasso) and L2 (Ridge) regularization**, Logistic Regression can **reduce overfitting**, especially in high-dimensional datasets.  \n",
        "\n",
        "---\n",
        "\n",
        "** Disadvantages of Logistic Regression**  \n",
        "\n",
        "**1. Assumes Linearity Between Log-Odds and Features**  \n",
        "Logistic Regression **assumes a linear relationship** between the independent variables and the **log-odds** of the outcome. If the data is non-linear, it performs poorly without feature transformations.  \n",
        "\n",
        "**2. Not Effective for Complex Relationships**  \n",
        "If the decision boundary is **non-linear**, Logistic Regression struggles, while models like **Decision Trees, Random Forests, or Neural Networks** perform better.  \n",
        "\n",
        "**3. Sensitive to Outliers**  \n",
        "Logistic Regression is affected by **outliers**, as they can **distort coefficient estimates**. Outliers should be handled using transformations or robust techniques.  \n",
        "\n",
        "**4. Poor Performance with Highly Correlated Features (Multicollinearity)**  \n",
        "If features are **highly correlated**, Logistic Regression's coefficients become unstable. Using **Ridge Regression (L2 regularization) or PCA** can mitigate this issue.  \n",
        "\n",
        "**5. Struggles with High-Dimensional Sparse Data**  \n",
        "When there are too many irrelevant features, Logistic Regression **overfits**. **L1 Regularization (Lasso)** helps **reduce dimensionality** by eliminating irrelevant features.  \n",
        "\n",
        "**6. Requires Large Sample Size for Reliable Estimates**  \n",
        "Logistic Regression performs best when there is **enough data** to estimate probabilities accurately. With a **small dataset**, models like **Naïve Bayes or Decision Trees** may work better.  \n",
        "\n",
        "#Ques 17. What are some use cases of Logistic Regression?\n",
        "Answer: **Use Cases of Logistic Regression**  \n",
        "\n",
        "Logistic Regression is widely used for **classification problems** in various industries due to its simplicity, interpretability, and efficiency. Here are some key real-world applications:  \n",
        "\n",
        "\n",
        "\n",
        "**1. Medical Diagnosis (Disease Prediction) **  \n",
        "- Logistic Regression is used to **predict the likelihood of diseases** such as **diabetes, cancer, and heart disease** based on patient data (e.g., blood pressure, cholesterol levels, age).  \n",
        "- Example: **Predicting whether a tumor is benign (0) or malignant (1).**  \n",
        "\n",
        "\n",
        "\n",
        "**2. Credit Scoring & Loan Approval **  \n",
        "- Banks and financial institutions use Logistic Regression to **predict the risk of loan defaults**.  \n",
        "- Example: Given a customer’s **income, credit score, and past loan history**, the model predicts whether they will **default (1) or not (0)**.  \n",
        "\n",
        "\n",
        "\n",
        "**3. Fraud Detection **  \n",
        "- Detecting **credit card fraud** and **insurance fraud** by analyzing transaction patterns.  \n",
        "- Example: A bank can use past transaction data to classify transactions as **fraudulent (1) or legitimate (0)**.  \n",
        "\n",
        "\n",
        "\n",
        "**4. Spam Email Detection **  \n",
        "- Logistic Regression can classify emails as **spam (1) or not spam (0)** by analyzing features like **keywords, sender information, and email structure**.  \n",
        "\n",
        "\n",
        "\n",
        "**5. Customer Churn Prediction **  \n",
        "- Companies use Logistic Regression to predict whether a **customer will leave (churn)** based on engagement metrics like **purchase frequency, customer complaints, and subscription renewals**.  \n",
        "- Example: A telecom company predicts whether a user will **switch to another service provider (1) or stay (0)**.  \n",
        "\n",
        "\n",
        "\n",
        "**6. Marketing Campaign Response Prediction **  \n",
        "- Businesses use Logistic Regression to predict **whether a customer will respond to an advertisement or not**.  \n",
        "- Example: Predicting whether an **email marketing campaign** will result in a **purchase (1) or no purchase (0)**.  \n",
        "\n",
        "\n",
        "\n",
        "**7. Employee Attrition Prediction **  \n",
        "- HR departments use Logistic Regression to **predict whether an employee will leave a company** based on factors like **job satisfaction, salary, and work environment**.  \n",
        "\n",
        "\n",
        "\n",
        "**8. Political Election Prediction **  \n",
        "- Logistic Regression is used to **predict voter preferences** based on demographic data and past voting patterns.  \n",
        "- Example: Predicting whether a voter will **support Candidate A (1) or Candidate B (0)**.  \n",
        "\n",
        "\n",
        "\n",
        "**9. COVID-19 Risk Prediction **  \n",
        "- Governments and healthcare institutions used Logistic Regression to **predict the likelihood of COVID-19 infection** based on symptoms, travel history, and prior exposure.  \n",
        "\n",
        "#Ques 18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "Answer: **Difference Between Softmax Regression and Logistic Regression**  \n",
        "\n",
        "Logistic Regression and Softmax Regression are both used for classification, but they differ in their applications and how they handle multiple classes.\n",
        "\n",
        "---\n",
        "\n",
        "**1. Definition**  \n",
        "- **Logistic Regression** is used for **binary classification (two classes, e.g., 0 or 1)**. It applies the **sigmoid function** to predict the probability of a sample belonging to Class 1.  \n",
        "- **Softmax Regression** (also called **Multinomial Logistic Regression**) is an extension of Logistic Regression used for **multiclass classification (more than two classes, e.g., Class 1, Class 2, Class 3, etc.)**. It uses the **softmax function** to assign probabilities to multiple classes.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Mathematical Equations**  \n",
        "\n",
        "**Logistic Regression (Binary Classification)**\n",
        "Logistic Regression applies the **sigmoid function**:  \n",
        "\\[\n",
        "P(Y=1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n)}}\n",
        "\\]  \n",
        "where the output is a probability between **0 and 1**, and the model classifies the sample as **1 if probability > 0.5**, otherwise **0**.\n",
        "\n",
        "---\n",
        "\n",
        "**Softmax Regression (Multiclass Classification)**\n",
        "Softmax Regression generalizes Logistic Regression to **K classes** using the **softmax function**:  \n",
        "\\[\n",
        "P(Y = k | X) = \\frac{e^{\\beta_k X}}{\\sum_{j=1}^{K} e^{\\beta_j X}}\n",
        "\\]  \n",
        "where:  \n",
        "- \\( P(Y = k | X) \\) is the probability of a sample belonging to class **k**.  \n",
        "- The denominator ensures that the sum of all class probabilities equals **1**.  \n",
        "- The model assigns the **class with the highest probability** to the sample.\n",
        "\n",
        "---\n",
        "\n",
        "**3. Decision Boundary**\n",
        "- **Logistic Regression** uses a **linear decision boundary** that separates **two** classes.  \n",
        "- **Softmax Regression** can handle **multiple linear decision boundaries** that separate **three or more** classes.\n",
        "\n",
        "---\n",
        "**4. Applications**\n",
        "| **Aspect** | **Logistic Regression** | **Softmax Regression** |\n",
        "|------------|----------------------|----------------------|\n",
        "| **Number of Classes** | Only **two** (binary classification) | **Three or more** (multiclass classification) |\n",
        "| **Function Used** | **Sigmoid function** | **Softmax function** |\n",
        "| **Decision Rule** | If \\( P(Y=1) > 0.5 \\), classify as 1 | Assign class with **highest probability** |\n",
        "| **Example Use Cases** | Fraud detection, spam filtering, medical diagnosis (yes/no) | Handwriting recognition, image classification, sentiment analysis |\n",
        "\n",
        "#Ques 19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "Answer: **Choosing Between One-vs-Rest (OvR) and Softmax for Multiclass Classification**  \n",
        "\n",
        "When performing **multiclass classification** (where the target variable has more than two categories), we can extend Logistic Regression using **One-vs-Rest (OvR)** or **Softmax Regression (Multinomial Logistic Regression)**. The choice depends on **dataset size, performance, and interpretability**.  \n",
        "\n",
        "---\n",
        "\n",
        "**1. One-vs-Rest (OvR) (One-vs-All Approach)**\n",
        "- Trains **K binary classifiers**, one for each class.  \n",
        "- Each classifier treats its class as **positive (1)** and all other classes as **negative (0)**.  \n",
        "- During prediction, each classifier outputs a probability, and the **class with the highest probability is chosen**.  \n",
        "\n",
        "**Pros:**  \n",
        "Works well with **small datasets**.  \n",
        "Computationally **efficient for a large number of classes**.  \n",
        "Can use **any binary classifier (not just Logistic Regression)**.  \n",
        "\n",
        "**Cons:**  \n",
        "Can be **inconsistent** since classifiers are trained separately.  \n",
        "Requires training **multiple models**, which can increase training time.  \n",
        "\n",
        "**2. Softmax Regression (Multinomial Logistic Regression)**\n",
        "- Trains **one single model** that predicts the probability of each class.  \n",
        "- Uses the **Softmax function** to normalize class probabilities:  \n",
        "  \\[\n",
        "  P(Y = k | X) = \\frac{e^{\\beta_k X}}{\\sum_{j=1}^{K} e^{\\beta_j X}}\n",
        "  \\]  \n",
        "- The **class with the highest probability** is selected.  \n",
        "\n",
        "**Pros:**  \n",
        "More **consistent** than OvR because all classes are considered together.  \n",
        "Works well for **balanced datasets**.  \n",
        "Computationally more **efficient for small or medium-sized datasets**.  \n",
        "\n",
        "**Cons:**  \n",
        "Can be **slow for very large datasets with many classes**.  \n",
        "Does **not work well when classes are highly imbalanced**.  \n",
        "\n",
        "#Ques 20. How do we interpret coefficients in Logistic Regression?\n",
        "Answer: In **Logistic Regression**, the coefficients represent the impact of each feature on the **log-odds** of the predicted outcome. Unlike **Linear Regression**, where coefficients directly indicate the change in the dependent variable, Logistic Regression coefficients affect the **logarithm of the odds** of an event occurring. The model follows the equation:  \n",
        "\n",
        "\\[\n",
        "\\log \\left(\\frac{P(Y=1)}{1 - P(Y=1)} \\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n\n",
        "\\]\n",
        "\n",
        "where each coefficient (\\(\\beta\\)) signifies how a **one-unit increase** in the corresponding feature (\\(X_i\\)) changes the **log-odds** of the outcome. To make interpretation easier, we convert coefficients into **odds ratios** using the exponential function:  \n",
        "\n",
        "\\[\n",
        "Odds \\ Ratio = e^{\\beta_i}\n",
        "\\]\n",
        "\n",
        "If an odds ratio is **greater than 1**, increasing that feature increases the likelihood of the event occurring, while an odds ratio **less than 1** decreases the likelihood. For example, if the coefficient for \"income\" is **0.5**, then \\( e^{0.5} \\approx 1.65 \\), meaning that a one-unit increase in income increases the odds of the event by **65%**. Conversely, a negative coefficient reduces the probability of the event. For categorical variables, coefficients indicate the effect of belonging to a specific category relative to the reference category. Standardizing features (mean=0, variance=1) makes coefficients comparable across different scales."
      ],
      "metadata": {
        "id": "RCR5TQWJdMjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PRACTICAL"
      ],
      "metadata": {
        "id": "ofSG66oTzH6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy."
      ],
      "metadata": {
        "id": "1AaiqXELzJGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Iris dataset as an example)\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = (data.target == 2).astype(int)  # Convert to binary classification (Class 2 vs. others)\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0MEkeO20p-A",
        "outputId": "87e4d24c-6fd6-406c-ace6-62d533353c66"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy."
      ],
      "metadata": {
        "id": "ugv4TdOb03Q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset (Iris dataset - converting it into a binary classification problem)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = (data.target == 2).astype(int)  # Convert to binary classification (Class 2 vs. others)\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train Logistic Regression model with L1 regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)  # 'liblinear' supports L1\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L1 Regularization: {accuracy:.4f}\")\n",
        "\n",
        "# Print model coefficients (some may be zero due to L1 regularization)\n",
        "print(\"Model Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTlS9ZkB06ny",
        "outputId": "a39450b4-cc8d-4071-eaad-2883c55dabfe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 Regularization: 0.9333\n",
            "Model Coefficients: [[-2.20211495 -2.83368819  3.20353301  3.72161443]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients"
      ],
      "metadata": {
        "id": "dXf7M8WY1Dot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset (Iris dataset - converting it into a binary classification problem)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = (data.target == 2).astype(int)  # Convert to binary classification (Class 2 vs. others)\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train Logistic Regression model with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', C=1.0)  # 'lbfgs' solver supports L2 regularization\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L2 Regularization: {accuracy:.4f}\")\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"Model Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X7ZsEHK1E_n",
        "outputId": "274cd162-b04a-4555-9055-f8b63a52119f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L2 Regularization: 1.0000\n",
            "Model Coefficients: [[-0.38904645 -0.62147609  2.7762982   2.09067085]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')."
      ],
      "metadata": {
        "id": "3QmiSwNz1KZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset (Iris dataset - converting it into a binary classification problem)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = (data.target == 2).astype(int)  # Convert to binary classification (Class 2 vs. others)\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train Logistic Regression model with Elastic Net Regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', C=1.0, l1_ratio=0.5)  # 'saga' supports Elastic Net\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with Elastic Net Regularization: {accuracy:.4f}\")\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"Model Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbpOQwIH1Oi1",
        "outputId": "6e62d585-ef9c-4245-9b99-e41ffdf1b967"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with Elastic Net Regularization: 1.0000\n",
            "Model Coefficients: [[-1.64668167 -1.64515962  2.68966493  2.29439491]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'"
      ],
      "metadata": {
        "id": "MT2p3LnS1T2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset (Iris dataset - which has 3 classes: 0, 1, 2)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Multiclass target variable (0, 1, 2)\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train Logistic Regression model with One-vs-Rest (OvR) strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', C=1.0)  # 'liblinear' supports OvR\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with One-vs-Rest (OvR): {accuracy:.4f}\")\n",
        "\n",
        "# Print model coefficients for each class\n",
        "print(\"Model Coefficients:\")\n",
        "for i, coef in enumerate(model.coef_):\n",
        "    print(f\"Class {i} Coefficients: {coef}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6617U3r1WgS",
        "outputId": "59ba51ce-cadd-408d-b369-da7de4f3f424"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with One-vs-Rest (OvR): 1.0000\n",
            "Model Coefficients:\n",
            "Class 0 Coefficients: [ 0.3711229   1.409712   -2.15210117 -0.95474179]\n",
            "Class 1 Coefficients: [ 0.49400451 -1.58897112  0.43717015 -1.11187838]\n",
            "Class 2 Coefficients: [-1.55895271 -1.58893375  2.39874554  2.15556209]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "fMkNU9Jx1blM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset (Iris dataset - which has 3 classes: 0, 1, 2)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Multiclass target variable (0, 1, 2)\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],  # L1 (Lasso) or L2 (Ridge) regularization\n",
        "    'solver': ['liblinear']  # 'liblinear' supports both L1 and L2\n",
        "}\n",
        "\n",
        "# Create Logistic Regression model\n",
        "log_reg = LogisticRegression(multi_class='ovr', max_iter=500)\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters and best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions using the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Best Model Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDSS9HPu1fbv",
        "outputId": "0a1ad9a0-386d-40fa-df8e-772fd964b19d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Model Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy."
      ],
      "metadata": {
        "id": "FNf-rvAi1kvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset (Iris dataset - which has 3 classes: 0, 1, 2)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Multiclass target variable (0, 1, 2)\n",
        "\n",
        "# Define Stratified K-Fold Cross-Validation\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold stratified split\n",
        "\n",
        "# Create Logistic Regression model\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=500)\n",
        "\n",
        "# Perform Cross-Validation\n",
        "cv_scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
        "\n",
        "# Print accuracy for each fold\n",
        "print(f\"Accuracy for each fold: {cv_scores}\")\n",
        "\n",
        "# Print average accuracy\n",
        "print(f\"Average Accuracy: {np.mean(cv_scores):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL54G_142Gpr",
        "outputId": "22498843-9627-40bf-922e-58d801a3e16e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for each fold: [0.96666667 1.         0.9        0.93333333 1.        ]\n",
            "Average Accuracy: 0.9600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy."
      ],
      "metadata": {
        "id": "RtzukpBD2Kw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload 'dataset.xlsx' manually\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "KA1qgdOD496Y",
        "outputId": "57bde2c7-0e9b-4abc-cb4d-21010d9a6481"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dec19bb8-65c8-47ef-b3ab-2c45fac0365d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-dec19bb8-65c8-47ef-b3ab-2c45fac0365d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dataset.xlsx to dataset.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"dataset.xlsx\")\n",
        "print(df.head())  # Verify dataset loading\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjxJ3iWr5nCd",
        "outputId": "d0440a06-cd98-4ee1-c419-bbced84a7354"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Age  EstimatedSalary  Purchased\n",
            "0   35            20000          0\n",
            "1   40            60000          1\n",
            "2   50            80000          1\n",
            "3   23            15000          0\n",
            "4   30            30000          0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from CSV file\n",
        "def load_data(csv_file):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    X = df.iloc[:, :-1].values  # Features (all columns except last)\n",
        "    y = df.iloc[:, -1].values   # Target (last column)\n",
        "    return X, y\n",
        "\n",
        "# File path (modify as needed)\n",
        "csv_file = 'dataset.csv'\n",
        "X, y = load_data(csv_file)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create and train Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=5000)\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = logreg.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "-x862oxg5r7u",
        "outputId": "abcb93c4-b798-441c-92c0-0a4387e358f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-436c5a02f2c8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# File path (modify as needed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcsv_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dataset.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Split dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-436c5a02f2c8>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(csv_file)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load dataset from CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m  \u001b[0;31m# Features (all columns except last)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m   \u001b[0;31m# Target (last column)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "FjmdjRrB52fE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (using a built-in dataset for demonstration)\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Prepare the data\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_dist = {\n",
        "    'C': np.logspace(-4, 4, 20),  # Regularization strength\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', None],  # Regularization type\n",
        "    'solver': ['liblinear', 'saga', 'lbfgs', 'newton-cg', 'sag']  # Optimization algorithms\n",
        "}\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=10000, random_state=42)\n",
        "\n",
        "# Use RandomizedSearchCV to find the optimal hyperparameters\n",
        "random_search = RandomizedSearchCV(estimator=log_reg, param_distributions=param_dist,\n",
        "                                   n_iter=20, cv=5, scoring='accuracy', random_state=42, n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best model\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Predict the labels for the test set using the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f'Best Parameters: {best_params}')\n",
        "print(f'Accuracy of Best Model: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgFR5VivMNOT",
        "outputId": "1995475b-f49e-4b4c-f533-670b86a97448"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver newton-cg supports only 'l2' or None penalties, got elasticnet penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver sag supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver sag supports only 'l2' or None penalties, got elasticnet penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1203, in fit\n",
            "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
            "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.90699367 0.96981013        nan        nan        nan        nan\n",
            " 0.90699367        nan 0.90702532 0.90702532 0.90696203 0.90702532\n",
            " 0.90702532        nan        nan        nan 0.95471519 0.90702532\n",
            "        nan 0.95971519]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 78.47599703514607}\n",
            "Accuracy of Best Model: 0.9766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy."
      ],
      "metadata": {
        "id": "V398YVmspSAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset (using the Iris dataset for demonstration)\n",
        "data = load_iris()\n",
        "\n",
        "# Prepare the data\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with One-vs-One (OvO) strategy\n",
        "log_reg_ovo = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=10000, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "log_reg_ovo.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = log_reg_ovo.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'One-vs-One (OvO) Logistic Regression Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS4L_ZqnpSTL",
        "outputId": "a47d3da4-4ed6-4a70-9973-854cd67d6838"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One (OvO) Logistic Regression Accuracy: 0.9778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification"
      ],
      "metadata": {
        "id": "0LXTNgxbpjao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the feature values\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print accuracy and classification report\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "ozS2VUTQArNW",
        "outputId": "1cdede58-35ef-4ca6-cd87-1ee5324ed595"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.83\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.84      0.82        89\n",
            "           1       0.87      0.82      0.84       111\n",
            "\n",
            "    accuracy                           0.83       200\n",
            "   macro avg       0.83      0.83      0.83       200\n",
            "weighted avg       0.83      0.83      0.83       200\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score."
      ],
      "metadata": {
        "id": "oYU_ydC0Asan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the feature values\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xsZlIDqAvdK",
        "outputId": "1097963d-3a1a-490f-e168-062daf96a2f0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.83\n",
            "Precision: 0.87\n",
            "Recall: 0.82\n",
            "F1-Score: 0.84\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.84      0.82        89\n",
            "           1       0.87      0.82      0.84       111\n",
            "\n",
            "    accuracy                           0.83       200\n",
            "   macro avg       0.83      0.83      0.83       200\n",
            "weighted avg       0.83      0.83      0.83       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance."
      ],
      "metadata": {
        "id": "tjJ-u6y5A5zB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the feature values\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model with class weights\n",
        "model = LogisticRegression(class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPYM1f2VA_pn",
        "outputId": "fe886b71-5968-4d05-d067-3602c03c612e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.86\n",
            "Precision: 0.48\n",
            "Recall: 0.88\n",
            "F1-Score: 0.62\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.86      0.92       175\n",
            "           1       0.48      0.88      0.62        25\n",
            "\n",
            "    accuracy                           0.86       200\n",
            "   macro avg       0.73      0.87      0.77       200\n",
            "weighted avg       0.92      0.86      0.88       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance."
      ],
      "metadata": {
        "id": "ETPq6bc2BPhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load Titanic dataset\n",
        "data_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(data_url)\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "target = 'Survived'\n",
        "\n",
        "df = df[features + [target]]\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])\n",
        "df['Fare'] = imputer.fit_transform(df[['Fare']])\n",
        "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Convert categorical variables\n",
        "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgckPSIZBRGu",
        "outputId": "ced8114e-9e0c-4a8e-cb30-ce9184119e54"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.81\n",
            "Precision: 0.79\n",
            "Recall: 0.74\n",
            "F1-Score: 0.76\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84       105\n",
            "           1       0.79      0.74      0.76        74\n",
            "\n",
            "    accuracy                           0.81       179\n",
            "   macro avg       0.81      0.80      0.80       179\n",
            "weighted avg       0.81      0.81      0.81       179\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-e607a188159e>:26: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Age'] = imputer.fit_transform(df[['Age']])\n",
            "<ipython-input-9-e607a188159e>:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Fare'] = imputer.fit_transform(df[['Fare']])\n",
            "<ipython-input-9-e607a188159e>:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
            "<ipython-input-9-e607a188159e>:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling."
      ],
      "metadata": {
        "id": "vp2LvZQyDr13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load Titanic dataset\n",
        "data_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(data_url)\n",
        "\n",
        "# Select relevant features and target variable\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "target = 'Survived'\n",
        "\n",
        "df = df[features + [target]]\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])\n",
        "df['Fare'] = imputer.fit_transform(df[['Fare']])\n",
        "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Convert categorical variables\n",
        "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression without scaling\n",
        "model_no_scaling = LogisticRegression()\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with scaling\n",
        "model_scaled = LogisticRegression()\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Compare accuracy\n",
        "print(f\"Accuracy without Scaling: {accuracy_no_scaling:.2f}\")\n",
        "print(f\"Accuracy with Scaling: {accuracy_scaled:.2f}\")\n",
        "\n",
        "# Compute evaluation metrics for scaled model\n",
        "precision = precision_score(y_test, y_pred_scaled)\n",
        "recall = recall_score(y_test, y_pred_scaled)\n",
        "f1 = f1_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print evaluation metrics for scaled model\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "print(\"Classification Report (Scaled Model):\")\n",
        "print(classification_report(y_test, y_pred_scaled))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7urcBGX2DwEe",
        "outputId": "45cb64dc-9ee8-4733-d734-e7efce846721"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Scaling: 0.80\n",
            "Accuracy with Scaling: 0.81\n",
            "Precision: 0.79\n",
            "Recall: 0.74\n",
            "F1-Score: 0.76\n",
            "Classification Report (Scaled Model):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84       105\n",
            "           1       0.79      0.74      0.76        74\n",
            "\n",
            "    accuracy                           0.81       179\n",
            "   macro avg       0.81      0.80      0.80       179\n",
            "weighted avg       0.81      0.81      0.81       179\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-c432f365a2e7>:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score."
      ],
      "metadata": {
        "id": "66jxw30FD4WI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset (using a built-in dataset for demonstration)\n",
        "# Replace this with your dataset loading logic\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Prepare the data\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "log_reg = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict the probabilities for the test set\n",
        "y_prob = log_reg.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(f'ROC-AUC Score: {roc_auc:.4f}')\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "MS8cRy5ND8AE",
        "outputId": "52c049d0-2170-4615-eb23-6fa5ae19378a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9968\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy."
      ],
      "metadata": {
        "id": "v4tifcEBEK0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset (using a built-in dataset for demonstration)\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Prepare the data\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with a custom C value (learning rate)\n",
        "log_reg = LogisticRegression(C=0.5, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g2ry6vkEN3b",
        "outputId": "d419b526-175b-4dad-c443-3e0588bf05ea"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 18. Write a Python program to train Logistic Regression and identify important features based on model coefficients."
      ],
      "metadata": {
        "id": "G1jxRKffEh4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset (using a built-in dataset for demonstration)\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Prepare the data\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Feature names\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "log_reg = LogisticRegression(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Get the model coefficients\n",
        "coefficients = log_reg.coef_[0]\n",
        "\n",
        "# Create a DataFrame to hold the feature names and their corresponding coefficients\n",
        "coeff_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
        "\n",
        "# Sort the features based on their absolute coefficient values\n",
        "coeff_df['AbsCoefficient'] = coeff_df['Coefficient'].abs()\n",
        "coeff_df = coeff_df.sort_values(by='AbsCoefficient', ascending=False)\n",
        "\n",
        "# Display the important features\n",
        "print(\"Important Features based on model coefficients:\")\n",
        "print(coeff_df[['Feature', 'Coefficient']])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0hMsxdKEsGv",
        "outputId": "6d5a2870-f0a2-4082-c9d0-007c036c429d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Important Features based on model coefficients:\n",
            "                    Feature  Coefficient\n",
            "20             worst radius     1.034079\n",
            "0               mean radius     1.014602\n",
            "26          worst concavity    -0.683038\n",
            "25        worst compactness    -0.548623\n",
            "21            worst texture    -0.545224\n",
            "1              mean texture     0.489008\n",
            "11            texture error     0.449482\n",
            "6            mean concavity    -0.255146\n",
            "2            mean perimeter     0.241038\n",
            "22          worst perimeter    -0.231764\n",
            "27     worst concave points    -0.202421\n",
            "28           worst symmetry    -0.184081\n",
            "5          mean compactness    -0.181180\n",
            "7       mean concave points    -0.109807\n",
            "13               area error    -0.108767\n",
            "12          perimeter error     0.066897\n",
            "24         worst smoothness    -0.066595\n",
            "8             mean symmetry    -0.056481\n",
            "16          concavity error    -0.052434\n",
            "29  worst fractal dimension    -0.050582\n",
            "15        compactness error    -0.038198\n",
            "4           mean smoothness    -0.037016\n",
            "10             radius error     0.033218\n",
            "3                 mean area    -0.017442\n",
            "17     concave points error    -0.013995\n",
            "18           symmetry error    -0.013689\n",
            "23               worst area    -0.011562\n",
            "9    mean fractal dimension    -0.011323\n",
            "19  fractal dimension error    -0.003544\n",
            "14         smoothness error    -0.003488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score."
      ],
      "metadata": {
        "id": "Vx6JxKMZE08B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset (using a built-in dataset for demonstration)\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Prepare the data\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=10000, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Calculate Cohen's Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(f'Cohen\\'s Kappa Score: {kappa_score:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV8_GUAyE4pK",
        "outputId": "5717e52c-1ece-493f-dfaf-685cb4264ca6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.9497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classificatio."
      ],
      "metadata": {
        "id": "S4HreIncE9wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset (using a built-in dataset for demonstration)\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Prepare the data\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=10000, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict the probabilities for the positive class (class=1)\n",
        "y_prob = log_reg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate Precision-Recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='blue', label='Precision-Recall curve')\n",
        "plt.fill_between(recall, precision, color='lightblue', alpha=0.4)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='lower left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "CBFWJwVJFqxK",
        "outputId": "0adca64f-42cd-4d90-f5e7-ecae927efd03"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy."
      ],
      "metadata": {
        "id": "yqKQSm8XF0Rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset (using a built-in dataset for demonstration)\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Prepare the data\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# List of solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Dictionary to store the accuracy for each solver\n",
        "accuracy_dict = {}\n",
        "\n",
        "# Train and evaluate the model for each solver\n",
        "for solver in solvers:\n",
        "    # Initialize the Logistic Regression model with the current solver\n",
        "    log_reg = LogisticRegression(solver=solver, random_state=42, max_iter=10000)\n",
        "\n",
        "    # Train the model\n",
        "    log_reg.fit(X_train, y_train)\n",
        "\n",
        "    # Predict the labels for the test set\n",
        "    y_pred = log_reg.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Store the accuracy\n",
        "    accuracy_dict[solver] = accuracy\n",
        "\n",
        "# Convert the accuracy results to a DataFrame for better visualization\n",
        "accuracy_df = pd.DataFrame(list(accuracy_dict.items()), columns=['Solver', 'Accuracy'])\n",
        "\n",
        "# Display the results\n",
        "print(accuracy_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "levIC3iXF3IQ",
        "outputId": "5012acae-7cf1-431a-a375-c80cac0f91d9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Solver  Accuracy\n",
            "0  liblinear  0.964912\n",
            "1       saga  0.964912\n",
            "2      lbfgs  0.976608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)."
      ],
      "metadata": {
        "id": "8x1cERDIF_xC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset (using a built-in dataset for demonstration)\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Prepare the data\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=10000, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f'Matthews Correlation Coefficient (MCC): {mcc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGc42JSqGEaG",
        "outputId": "bbe0eb2d-a36d-402f-c8c6-683eb1d59d4d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.9497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling."
      ],
      "metadata": {
        "id": "ghbsmTmjGJ51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset (using a built-in dataset for demonstration)\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Prepare the data\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression on raw data\n",
        "log_reg_raw = LogisticRegression(max_iter=10000, random_state=42)\n",
        "log_reg_raw.fit(X_train, y_train)\n",
        "y_pred_raw = log_reg_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Standardize the data using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression on standardized data\n",
        "log_reg_scaled = LogisticRegression(max_iter=10000, random_state=42)\n",
        "log_reg_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = log_reg_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Compare the accuracy for both raw and standardized data\n",
        "print(f'Accuracy on raw data: {accuracy_raw:.4f}')\n",
        "print(f'Accuracy on standardized data: {accuracy_scaled:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96dTvhiYa1th",
        "outputId": "6815ec42-1ac8-435b-ac76-38d8fbc9b226"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data: 0.9766\n",
            "Accuracy on standardized data: 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation."
      ],
      "metadata": {
        "id": "YEcwtAIcbLSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (using a built-in dataset for demonstration)\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Prepare the data\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=10000, random_state=42)\n",
        "\n",
        "# Define the parameter grid for C (regularization strength)\n",
        "param_grid = {'C': np.logspace(-4, 4, 20)}  # Exploring a range of C values in log scale\n",
        "\n",
        "# Use GridSearchCV to find the optimal C with cross-validation\n",
        "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best C and the corresponding accuracy\n",
        "best_C = grid_search.best_params_['C']\n",
        "best_accuracy = grid_search.best_score_\n",
        "\n",
        "# Train the Logistic Regression model with the optimal C\n",
        "optimal_log_reg = LogisticRegression(C=best_C, max_iter=10000, random_state=42)\n",
        "optimal_log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = optimal_log_reg.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f'Optimal C: {best_C}')\n",
        "print(f'Best cross-validation accuracy: {best_accuracy:.4f}')\n",
        "print(f'Test accuracy with optimal C: {test_accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fS0iaTgbONt",
        "outputId": "3dec13af-c734-483e-f0c2-f1f98fa52318"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal C: 545.5594781168514\n",
            "Best cross-validation accuracy: 0.9698\n",
            "Test accuracy with optimal C: 0.9766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ques 25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions."
      ],
      "metadata": {
        "id": "u6nLqySmbRgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (using a built-in dataset for demonstration)\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Prepare the data\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=10000, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Save the trained model using joblib\n",
        "joblib.dump(log_reg, 'logistic_regression_model.joblib')\n",
        "print(\"Model saved successfully!\")\n",
        "\n",
        "# Load the model back\n",
        "loaded_model = joblib.load('logistic_regression_model.joblib')\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy of loaded model: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLFNlzZ5bWYU",
        "outputId": "0c69a6e4-bf58-42aa-9fce-ef78efa07a45"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully!\n",
            "Model loaded successfully!\n",
            "Accuracy of loaded model: 0.9766\n"
          ]
        }
      ]
    }
  ]
}